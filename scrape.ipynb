{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.hbokennisbank.nl/searchresult?q=&sort-order=date&date-from=&date-until=&t-0-k=hbo%3Aproduct&t-0-v=info%3Aeu-repo%2Fsemantics%2FbachelorThesis&t-0-v=info%3Aeu-repo%2Fsemantics%2FmasterThesis&t-0-v=info%3Aeu-repo%2Fsemantics%2FassociateDegree&c=2&has-link=yes'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = \"https://www.hbokennisbank.nl\"\n",
    "search_url = \"/searchresult?q=&sort-order=date&date-from=&date-until=&t-0-k=hbo%3Aproduct&t-0-v=info%3Aeu-repo%2Fsemantics%2FbachelorThesis&t-0-v=info%3Aeu-repo%2Fsemantics%2FmasterThesis&t-0-v=info%3Aeu-repo%2Fsemantics%2FassociateDegree&c=2&has-link=yes\"\n",
    "base_url + search_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_url = base_url + search_url\n",
    "response = session.get(current_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "sublinks = soup.find_all('a', href=True)\n",
    "detail_links = get_details(sublinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_url = detail_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Invloed_van_big_data_analytics_op_organisatorische_prestaties_binnen_Nederlandse_overheidsorganisaties'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = session.get(base_url + detail_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "div_tag = soup.find('div', class_='detail__header__column')\n",
    "div_tag.find('h1').text.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class ThesisScraper:\n",
    "    def __init__(self, base_url: str, search_url: str, datafolder: Path) -> None:\n",
    "        self.base_url = base_url\n",
    "        self.search_url = search_url\n",
    "        self.datafolder = datafolder\n",
    "        self.info = []\n",
    "\n",
    "    def __call__(self) -> list:\n",
    "        self.run()\n",
    "\n",
    "    def download(self, max_downloads: int = 1000):\n",
    "        for i in tqdm(range(max_downloads)):\n",
    "            self.download_file(self.info[i])\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'ThesisScraper(base_url={self.base_url})'\n",
    "\n",
    "    def run(self):\n",
    "        session = requests.Session()\n",
    "        info = []\n",
    "\n",
    "        for i in tqdm(range(100)):\n",
    "            current_url = self.base_url + self.search_url\n",
    "            # logger.info(f\"using url: {current_url}\")\n",
    "            response = session.get(current_url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            sublinks = soup.find_all('a', href=True)\n",
    "            detail_links = self.get_details(sublinks)\n",
    "            for detail_url in detail_links:\n",
    "                download_url, features = self.get_subpage_info(detail_url, session)\n",
    "                self.info.append([download_url, features])\n",
    "            pagination = soup.find_all('a', class_='search__body__pagination__arrow')\n",
    "            if 'is-disabled' not in pagination[1].attrs['class']:\n",
    "                self.search_url = pagination[1]['href']\n",
    "            else:\n",
    "                logger.info(f'No more pages to scrape at page {i+1}.')\n",
    "                break\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_details(subpage_links: list) -> list:\n",
    "        detail_links = []\n",
    "        for link in subpage_links:\n",
    "            href = link['href']\n",
    "            if href.startswith('/details'):\n",
    "                detail_links.append(href)\n",
    "        return detail_links\n",
    "\n",
    "    def get_subpage_info(self, detail_url: str, session: requests.Session) -> tuple:\n",
    "        response = session.get(self.base_url + detail_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        download_url = soup.find('a', class_='detail__header__button')\n",
    "        features = self._obtain_features_from_table(soup)\n",
    "        features[\"title\"] = self._get_title(soup)\n",
    "\n",
    "        if download_url and 'href' in download_url.attrs:\n",
    "            download_url = self.base_url + download_url['href']\n",
    "\n",
    "        return download_url, features\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_title(soup) -> str:\n",
    "        div_tag = soup.find('div', class_='detail__header__column')\n",
    "        if div_tag:\n",
    "            title = div_tag.find('h1').text.replace(' ', '_')\n",
    "        return title\n",
    "\n",
    "    def download_file(self, info):\n",
    "        url = info[0]\n",
    "        feat = info[1]\n",
    "        filename = Path(feat[\"title\"]).with_suffix('.pdf')\n",
    "        if feat[\"organisatie\"] and feat[\"opleiding\"]:\n",
    "            subdir = Path(feat[\"organisatie\"].replace(' ', '_')) / Path(feat[\"opleiding\"].replace(' ', '_'))\n",
    "        else:\n",
    "            subdir = \"\"\n",
    "        filepath = Path(self.datafolder) / subdir\n",
    "        if not filepath.exists():\n",
    "            filepath.mkdir(parents=True)\n",
    "        path = filepath / filename\n",
    "        if path.exists():\n",
    "            logger.info(f\"skipping {path}, already on disk\")\n",
    "        else:\n",
    "            logger.info(f'Downloading {path}')\n",
    "            response = session.get(url, stream=True)\n",
    "            with (path).open('wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=128):\n",
    "                    f.write(chunk)\n",
    "\n",
    "    @staticmethod\n",
    "    def _obtain_features_from_table(soup) -> dict:\n",
    "        rows = soup.find_all('tr')\n",
    "        organisatie = None\n",
    "        opleiding = None\n",
    "\n",
    "        # Iterate through each row and check the label in the first cell\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 2:  # Ensure there are at least two cells in the row\n",
    "                label = cells[0].text.strip()\n",
    "                value = cells[1].text.strip()\n",
    "\n",
    "                # Check for 'Organisatie' and 'Opleiding' labels\n",
    "                if label == 'Organisatie':\n",
    "                    organisatie = value\n",
    "                elif label == 'Opleiding':\n",
    "                    opleiding = value\n",
    "        return {\"organisatie\" : organisatie, \"opleiding\" : opleiding}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [02:05<00:02,  1.30s/it]\u001b[32m2023-11-14 20:19:48.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mNo more pages to scrape at page 99.\u001b[0m\n",
      " 98%|█████████▊| 98/100 [02:06<00:02,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.hbokennisbank.nl\"\n",
    "search_url = \"/searchresult?q=&sort-order=date&date-from=&date-until=&t-0-k=hbo%3Aproduct&t-0-v=info%3Aeu-repo%2Fsemantics%2FbachelorThesis&t-0-v=info%3Aeu-repo%2Fsemantics%2FmasterThesis&t-0-v=info%3Aeu-repo%2Fsemantics%2FassociateDegree&c=2&has-link=yes\"\n",
    "scraper = ThesisScraper(base_url, search_url, datafolder=Path('downloads'))\n",
    "scraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.download(max_downloads=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
